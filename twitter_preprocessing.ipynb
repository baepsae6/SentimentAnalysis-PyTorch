{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = pd.read_csv('positive.csv', delimiter=';', header=None)\n",
    "data_neg = pd.read_csv('negative.csv', delimiter=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos[3]\n",
    "data_neg = data_neg[3]\n",
    "data_pos = pd.DataFrame(data_pos)\n",
    "data_neg = pd.DataFrame(data_neg)\n",
    "data_pos = data_pos.rename(index=int, columns={3: \"text\"})\n",
    "data_neg = data_neg.rename(index=int, columns={3: \"text\"})\n",
    "data_pos['labels'] = 1\n",
    "data_neg['labels'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114911, 111923)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_pos), len(data_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stgmadina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/stgmadina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import (sent_tokenize as splitter,wordpunct_tokenize as tokenizer)\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [tokenizer(sentence) for sentence in splitter(text)]\n",
    "\n",
    "def flatten(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "def tokenize_flatten_df(row, field):\n",
    "    return flatten(tokenize(row[field]))\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"(https?\\://)\\S+\", \"\", text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r\"@[^:| ]+:? ?\", \"\", text)\n",
    "\n",
    "def remove_rt(text):\n",
    "    if text.lower().startswith(\"rt\"):\n",
    "        return text[3:].strip()\n",
    "    return text\n",
    "\n",
    "def remove_urls_mentions_rt_df(row, field):\n",
    "    return remove_rt(remove_mentions(remove_urls(row[field])))\n",
    "\n",
    "def replace_hashtags_from_list(tokens_list):\n",
    "    return [token for token in tokens_list if token != \"#\"]\n",
    "\n",
    "def remove_digits(tokens_list):\n",
    "    return [token for token in tokens_list \n",
    "                if not re.match(r\"[-+]?\\d+(\\.[0-9]*)?$\", token)]\n",
    "\n",
    "def remove_containing_non_alphanum(tokens_list):\n",
    "    return [re.sub(r'[^а-яА-Я\\(\\)\\:]', \"\", token) for token in tokens_list]\n",
    "                \n",
    "def lowercase_list(tokens_list):\n",
    "    return [token.lower() for token in tokens_list]\n",
    "\n",
    "def remove_stopwords(tokens_list):\n",
    "    return [token for token in tokens_list\n",
    "                if not token in stopwords.words(u'russian')]\n",
    "\n",
    "def clean_tokens(row, field):\n",
    "    return replace_hashtags_from_list(\n",
    "        remove_digits(\n",
    "            remove_containing_non_alphanum(\n",
    "                lowercase_list(remove_stopwords(row[field])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_pos, data_neg], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_cleaned_from_url_mentions_rt'] = \\\n",
    "    data.apply(\n",
    "        lambda row: remove_urls_mentions_rt_df (row, 'text'),\n",
    "        axis=1)\n",
    "\n",
    "data['text_tokenized'] = \\\n",
    "    data.apply(\n",
    "        lambda row:\n",
    "            tokenize_flatten_df (row, 'text_cleaned_from_url_mentions_rt'),\n",
    "        axis=1)\n",
    "\n",
    "data['text_tokenized_cleaned'] = \\\n",
    "    data.apply(\n",
    "        lambda row:\n",
    "            clean_tokens (row, 'text_tokenized'),\n",
    "        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>text_cleaned_from_url_mentions_rt</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_tokenized_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>1</td>\n",
       "      <td>хоть я и школота, но поверь, у нас то же самое...</td>\n",
       "      <td>[хоть, я, и, школота, ,, но, поверь, ,, у, нас...</td>\n",
       "      <td>[школота, , поверь, , самое, :, , общество, пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>[Да, ,, все, -, таки, он, немного, похож, на, ...</td>\n",
       "      <td>[да, , , таки, немного, похож, , но, мальчик, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Ну ты идиотка) я испугалась за тебя!!!</td>\n",
       "      <td>[Ну, ты, идиотка, ), я, испугалась, за, тебя, ...</td>\n",
       "      <td>[ну, идиотка, ), испугалась, , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Кто то в углу сидит и погибает от голода, а м...</td>\n",
       "      <td>[\", Кто, то, в, углу, сидит, и, погибает, от, ...</td>\n",
       "      <td>[, кто, углу, сидит, погибает, голода, , ещ, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>1</td>\n",
       "      <td>Вот что значит страшилка :D\\nНо блин,посмотрев...</td>\n",
       "      <td>[Вот, что, значит, страшилка, :, D, Но, блин, ...</td>\n",
       "      <td>[вот, значит, страшилка, :, , но, блин, , посм...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels  \\\n",
       "0  @first_timee хоть я и школота, но поверь, у на...       1   \n",
       "1  Да, все-таки он немного похож на него. Но мой ...       1   \n",
       "2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...       1   \n",
       "3  RT @digger2912: \"Кто то в углу сидит и погибае...       1   \n",
       "4  @irina_dyshkant Вот что значит страшилка :D\\nН...       1   \n",
       "\n",
       "                   text_cleaned_from_url_mentions_rt  \\\n",
       "0  хоть я и школота, но поверь, у нас то же самое...   \n",
       "1  Да, все-таки он немного похож на него. Но мой ...   \n",
       "2             Ну ты идиотка) я испугалась за тебя!!!   \n",
       "3  \"Кто то в углу сидит и погибает от голода, а м...   \n",
       "4  Вот что значит страшилка :D\\nНо блин,посмотрев...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [хоть, я, и, школота, ,, но, поверь, ,, у, нас...   \n",
       "1  [Да, ,, все, -, таки, он, немного, похож, на, ...   \n",
       "2  [Ну, ты, идиотка, ), я, испугалась, за, тебя, ...   \n",
       "3  [\", Кто, то, в, углу, сидит, и, погибает, от, ...   \n",
       "4  [Вот, что, значит, страшилка, :, D, Но, блин, ...   \n",
       "\n",
       "                              text_tokenized_cleaned  \n",
       "0  [школота, , поверь, , самое, :, , общество, пр...  \n",
       "1  [да, , , таки, немного, похож, , но, мальчик, ...  \n",
       "2                   [ну, идиотка, ), испугалась, , ]  \n",
       "3  [, кто, углу, сидит, погибает, голода, , ещ, ,...  \n",
       "4  [вот, значит, страшилка, :, , но, блин, , посм...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(['text', 'text_tokenized', 'text_cleaned_from_url_mentions_rt'], axis=1)\n",
    "data.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['text'] = [[i for i in d if i!=''] for d in data['text']]\n",
    "data['text'] = [\" \".join(d) for d in data['text']]\n",
    "data['text'] = [d.strip() for d in data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_rows = []\n",
    "for i , t in enumerate(data['text'].values):\n",
    "    if t == '':\n",
    "        empty_rows.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.index[empty_rows], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_tokenized'] = \\\n",
    "    data.apply(\n",
    "        lambda row:\n",
    "            tokenize_flatten_df (row, 'text'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['text'], axis=1)\n",
    "data.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter_prep_data_brackets.pickle', 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
