{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from vocab import Vocab\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from nltk import (sent_tokenize as splitter, wordpunct_tokenize as tokenizer)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from embeddings import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(object): \n",
    "    def __init__(self):\n",
    "        self.dataset = self.read_json('twitter_prep_data.json') \n",
    "        self.word_list = []\n",
    "        self.d = Embeddings().load_embeddings()\n",
    "        \n",
    "    def read_json(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            dataset = pd.DataFrame.from_dict(data) \n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def create_vocab(self):\n",
    "        for s in tqdm(self.dataset['text'].values):\n",
    "            self.word_list += s\n",
    "        word_counter = Counter(self.word_list)\n",
    "        vocab = Vocab(word_counter, min_freq=10)\n",
    "        return vocab\n",
    "\n",
    "\n",
    "    class TwitterDataset(Dataset):\n",
    "        def __init__(self, data, vocab):\n",
    "            self.vocab = vocab\n",
    "            self.data = data\n",
    "            self.text = self.data['text'].values\n",
    "            self.label = self.data['label'].values\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            text = self.text[index]\n",
    "            label = self.label[index]\n",
    "            text = self.vocab.sent2idx(text)\n",
    "            sample = {'label': label, 'text': text}\n",
    "            return sample\n",
    "\n",
    "        def collate_fn(self, dicts): \n",
    "            pad_token = 0\n",
    "            sents_padded = []\n",
    "            corpus_size = len(dicts)\n",
    "            len_text_list = [len(d['text']) for d in dicts]\n",
    "            text_list = [d['text'] for d in dicts]\n",
    "            labels = [i['label'] for i in dicts]\n",
    "\n",
    "            sorted_len_text, sorted_text, sorted_labels = list(zip(*sorted(zip(len_text_list, text_list, labels), key=lambda x: x[0] ,reverse=True))) #sorts sentences in the reverse hierarchical order        \n",
    "            max_lens = sorted_len_text[0]\n",
    "\n",
    "            text_padded = [sorted_text[i] + [pad_token] * (max_lens - sorted_len_text[i]) for i in range(corpus_size)]\n",
    "            text_padded = torch.LongTensor(text_padded)\n",
    "            labels = torch.FloatTensor(sorted_labels)\n",
    "\n",
    "            return text_padded, labels, sorted_len_text\n",
    "\n",
    "\n",
    "    def create_train_dataset(self):\n",
    "        X_train, X_test = train_test_split(self.dataset, test_size=0.33, random_state=42)\n",
    "        vocab = self.create_vocab()\n",
    "        train_dataset = SentimentClassifier.TwitterDataset(X_train, vocab)\n",
    "        test_dataset = SentimentClassifier.TwitterDataset(X_test, vocab)\n",
    "        return train_dataset, test_dataset, vocab\n",
    "\n",
    "\n",
    "    def create_dataloaders(self, train_dataset, test_dataset):\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=32,\n",
    "                            shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "        val_dataloader = DataLoader(test_dataset, batch_size=32,\n",
    "                               shuffle=False, collate_fn=test_dataset.collate_fn)\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "    def create_pretrained_embeddings(self, vocab):\n",
    "        matrix_len = len(vocab._token2idx)\n",
    "        weights_matrix = np.zeros((matrix_len, 100))\n",
    "        words_found = 0\n",
    "        for i, word in enumerate(vocab._token2idx):\n",
    "            try: \n",
    "                weights_matrix[i] = self.d[word]\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.6, size=(100, ))\n",
    "        pretrained_embeddings = weights_matrix\n",
    "        pretrained_embeddings = torch.FloatTensor(pretrained_embeddings)\n",
    "        return pretrained_embeddings\n",
    "\n",
    "\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, vocab_size, embedding_dim,\n",
    "                     hidden_dim, output_dim, n_layers, \n",
    "                     bidirectional, dropout, pad_idx):\n",
    "\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)        \n",
    "            self.rnn = nn.LSTM(embedding_dim, \n",
    "                               hidden_dim, \n",
    "                               num_layers=n_layers, \n",
    "                               bidirectional=bidirectional, \n",
    "                               dropout=dropout)\n",
    "\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, text, text_lengths):\n",
    "            embedded = self.embedding(text)\n",
    "            if text_lengths == 0:\n",
    "                return\n",
    "            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)\n",
    "            packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1, :,:]), dim = 1)\n",
    "            return self.fc(hidden.squeeze(0))\n",
    "\n",
    "\n",
    "    def binary_accuracy(self, preds, y):\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float() \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def train(self, model, iterator, optimizer, criterion):\n",
    "        train_losses = []\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (text, label, len_text) in enumerate(iterator): \n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            predictions = model(text, len_text).squeeze(1)\n",
    "            loss = criterion(predictions, label.float())\n",
    "            acc = self.binary_accuracy(predictions, label.float())   \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                self.plot(1, batch_idx, train_losses)\n",
    "        return epoch_loss/len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "    def evaluate(self, model, iterator, criterion):\n",
    "        eval_losses = []\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for text, label, len_text in iterator:\n",
    "                text = text.to(device)\n",
    "                label = label.to(device)\n",
    "                predictions = model(text, len_text).squeeze(1)\n",
    "                loss = criterion(predictions, label.float())\n",
    "                eval_losses.append(loss.item())\n",
    "                acc = self.binary_accuracy(predictions, label.float())\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "        return epoch_loss/len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "    def plot(self, epoch, step, train_losses):\n",
    "        clear_output()\n",
    "        plt.title(f'Epochs {epoch}, step {step}')\n",
    "        plt.plot(train_losses)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def epoch_time(self, start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "    def make_all(self): \n",
    "        train_dataset, test_dataset, vocab = self.create_train_dataset()\n",
    "        train_dataloader, test_dataloader = self.create_dataloaders(train_dataset, test_dataset)\n",
    "        pretrained_embeddings = self.create_pretrained_embeddings(vocab)\n",
    "\n",
    "        INPUT_DIM = len(vocab)\n",
    "        EMBEDDING_DIM = 100\n",
    "        HIDDEN_DIM = 256\n",
    "        OUTPUT_DIM = 1\n",
    "        N_LAYERS = 2\n",
    "        BIDIRECTIONAL = True\n",
    "        DROPOUT = 0.5\n",
    "        PAD_IDX = 0\n",
    "\n",
    "        model = SentimentClassifier.RNN(INPUT_DIM,\n",
    "                    EMBEDDING_DIM,\n",
    "                    HIDDEN_DIM, \n",
    "                    OUTPUT_DIM, \n",
    "                    N_LAYERS, \n",
    "                    BIDIRECTIONAL, \n",
    "                    DROPOUT, \n",
    "                    0).to(device)\n",
    "        model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        return model, vocab\n",
    "    \n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        N_EPOCHS = 1\n",
    "        best_valid_loss = float('inf')\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_loss, train_acc = self.train(model, train_dataloader, optimizer, criterion)\n",
    "            valid_loss, valid_acc = self.evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentPredictor(object):\n",
    "    def __init__(self):\n",
    "        #self.clf = SentimentClassifier()\n",
    "        self.model, self.vocab = SentimentClassifier().make_all()\n",
    "\n",
    "    def predict_sentiment(self, sentence):\n",
    "        self.model.eval()\n",
    "        tokenized = [tokenizer(sentence) for sentence in splitter(sentence)]\n",
    "        print()\n",
    "        indexed = [self.vocab.sent2idx(tokenized[0])]\n",
    "        length = [len(indexed)]\n",
    "        tensor = torch.LongTensor(indexed).to(device)\n",
    "        prediction = torch.sigmoid(self.model(tensor, length))\n",
    "\n",
    "        return prediction.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
